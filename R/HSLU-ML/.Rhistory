View(x)
plot(x,col=ifelse(y>0,1,2))
legend("topleft",c('Positive','Negative'),col=seq(2),pch=1,text.col=seq(2))
data(iris)
head(iris, 3)
# first log transform the data
log.ir <- log(iris[, 1:4])
ir.species <- iris[, 5]
# apply PCA
# scale. = TRUE is highly advisable, but default is FALSE.
ir.pca <- prcomp(log.ir,
center = TRUE,
scale. = TRUE)
print(ir.pca)
plot(ir.pca, type = "l")
summary(ir.pca)
predict(ir.pca, newdata=tail(log.ir, 2))
library(devtools)
install_github("ggbiplot", "vqv")
library(ggbiplot)
g <- ggbiplot(ir.pca, obs.scale = 1, var.scale = 1,
groups = ir.species, ellipse = TRUE,
circle = TRUE)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal',
legend.position = 'top')
print(g)
biplot(ir.pca)
print(g)
credit <- read.csv("data/credit.csv")
View(credit)
library(caret)
install.packages("caret")
library(caret)
m <- train(default ~ ., data = credit, method = "C5.0")
m <- train(default ~ ., data = credit, method = "C5.0")
p <- predict(m, credit)
m <- train(default ~ ., data = credit, method = "C5.0")
install.packages("e1071")
m <- train(default ~ ., data = credit, method = "C5.0")
p <- predict(m, credit)
table(p, credit$default)
head(predict(m, credit))
head(predict(m, credit, type = "prob"))
ctrl <- trainControl(method = "cv", number = 10, selectionFunction = "oneSE")
grid <- expand.grid(.model = "tree", .trials = c(1, 5, 10, 15, 20, 25, 30, 35), .winnow = "FALSE")
grid
m <- train(default ~ ., data = credit, method = "C5.0", metric = "Kappa", trControl = ctrl, tuneGrid = grid)
m
library(ipred)
install.packages("ipred")
mybag <- bagging(default ~ ., data = credit, nbagg = 25)
library(ipred)
mybag <- bagging(default ~ ., data = credit, nbagg = 25)
credit_pred <- predict(mybag, credit)
table(credit_pred, credit$default)
library(caret)
ctrl <- trainControl(method = "cv", number = 10)
train(default ~ ., data = credit, method = "treebag", trControl = ctrl)
# Bagging
str(svmBag)
str(svmBag)
svmBag$fit
bagctrl <- bagControl(fit = svmBag$fit, predict = svmBag$pred, aggregate = svmBag$aggregate)
svmBag <- train(default ~ ., data = credit, "bag", trControl = ctrl, bagControl = bagctrl)
svmBag
# Random Forest
library(randomForest)
install.packages("randomForest")
library(randomForest)
rf <- randomForest(default ~ ., data = credit)
rf
library(caret)
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
grid_rf <- expand.grid(.mtry = c(2, 4, 8, 16))
m_rf <- train(default ~ ., data = credit, method = "rf", metric = "Kappa", trControl = ctrl, tuneGrid = grid_rf)
grid_c50 <- expand.grid(.model = "tree", .trials = c(10, 20, 30, 40), .winnow = "FALSE")
m_c50 <- train(default ~ ., data = credit, method = "C5.0", metric = "Kappa", trControl = ctrl, tuneGrid = grid_c50)
m_rf
m-c50
url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'
wine <- read.table(url)
head(wine)
wine <- read.table("data/winequality-white.csv")
wine <- read.csv("data/winequality-white.csv", sep=";")
head(wine)
barplot(table(wine$quality))
wine$taste <- ifelse(wine$quality < 6, 'bad', 'good')
wine$taste[wine$quality == 6] <- 'normal'
wine$taste <- as.factor(wine$taste)
head(wine)
table(wine$taste)
set.seed(123)
samp <- sample(nrow(wine), 0.6 * nrow(wine))
train <- wine[samp, ]
test <- wine[-samp, ]
model <- randomForest(taste ~ . - quality, data = train)
model
?randomForest
wine <- read.csv("data/winequality-white.csv", sep=";")
head(wine)
barplot(table(wine$quality))
# Classify into good, bad, and normal wines based on their quality.
wine$taste <- ifelse(wine$quality < 6, 'bad', 'good')
wine$taste[wine$quality == 6] <- 'normal'
wine$taste <- as.factor(wine$taste)
# Summary
table(wine$taste)
# Separate the data into testing (60%) and training (40%) sets.
set.seed(123)
samp <- sample(nrow(wine), 0.6 * nrow(wine))
train <- wine[samp, ]
test <- wine[-samp, ]
# Build the random forest model
library(randomForest)
model <- randomForest(taste ~ . - quality, data = train)
# Show the model configuration
model
# We can use ntree and mtry to specify the total number of trees to build (default = 500),
# and the number of predictors to randomly sample at each split respectively. See the R Documentation
?randomForest
# With the default values 500 trees were built, and the model randomly sampled 3 predictors at each split.
# It also shows a matrix containing prediction vs actual, as well as classification error for each class.
# Testing the model on the test data set
pred <- predict(model, newdata = test)
table(pred, test$taste)
# Accuracy is calculated as follows:
(482 + 252 + 667) / nrow(test)
samp <- sample(nrow(wine), 0.66 * nrow(wine))
train <- wine[samp, ]
test <- wine[-samp, ]
# Build the random forest model
library(randomForest)
model <- randomForest(taste ~ . - quality, data = train)
# Show the model configuration
model
mytable <- table(pred, test$taste)
pred <- predict(model, newdata = test)
table(pred, test$taste)
mytable <- table(pred, test$taste)
mytable[1,1]
mytable[2,2]
pred_table <- table(pred, test$taste)
# Accuracy is calculated as follows:
(pred_table[1,1] + pred_table[2,2] + pred_table[3,3]) / nrow(test)
head(wine)
model <- randomForest(taste ~ . - alcohol, data = train)
# Show the model configuration
model
# We can use ntree and mtry to specify the total number of trees to build (default = 500),
# and the number of predictors to randomly sample at each split respectively. See the R Documentation
?randomForest
# With the default values 500 trees were built, and the model randomly sampled 3 predictors at each split.
# It also shows a matrix containing prediction vs actual, as well as classification error for each class.
# Testing the model on the test data set
pred <- predict(model, newdata = test)
pred_table <- table(pred, test$taste)
# Accuracy is calculated as follows:
(pred_table[1,1] + pred_table[2,2] + pred_table[3,3]) / nrow(test)
pred_table
model <- randomForest(taste ~ . - fixed.acidty, data = train)
model <- randomForest(taste ~ . - 'fixed.acidty, data = train)
model <- randomForest(taste ~ . - 'fixed.acidty', data = train)
model <- randomForest(taste ~ . - pH, data = train)
# Show the model configuration
model
model <- randomForest(taste ~ . - quality, data = train)
# Show the model configuration
model
# We can use ntree and mtry to specify the total number of trees to build (default = 500),
# and the number of predictors to randomly sample at each split respectively. See the R Documentation
?randomForest
# With the default values 500 trees were built, and the model randomly sampled 3 predictors at each split.
# It also shows a matrix containing prediction vs actual, as well as classification error for each class.
# Testing the model on the test data set
pred <- predict(model, newdata = test)
pred_table <- table(pred, test$taste)
# Accuracy is calculated as follows:
(pred_table[1,1] + pred_table[2,2] + pred_table[3,3]) / nrow(test)
head(wine)
model <- randomForest(taste ~ . - quality + pH, data = train)
# Show the model configuration
model
pred <- predict(model, newdata = test)
pred_table <- table(pred, test$taste)
# Accuracy is calculated as follows:
(pred_table[1,1] + pred_table[2,2] + pred_table[3,3]) / nrow(test)
model <- randomForest(taste ~ . - quality + pH + sulphates, data = train)
# Show the model configuration
model
# We can use ntree and mtry to specify the total number of trees to build (default = 500),
# and the number of predictors to randomly sample at each split respectively. See the R Documentation
?randomForest
# With the default values 500 trees were built, and the model randomly sampled 3 predictors at each split.
# It also shows a matrix containing prediction vs actual, as well as classification error for each class.
# Testing the model on the test data set
pred <- predict(model, newdata = test)
pred_table <- table(pred, test$taste)
# Accuracy is calculated as follows:
(pred_table[1,1] + pred_table[2,2] + pred_table[3,3]) / nrow(test)
model <- randomForest(taste ~ . - quality + pH + sulphates + chlorides, data = train)
model
# We can use ntree and mtry to specify the total number of trees to build (default = 500),
# and the number of predictors to randomly sample at each split respectively. See the R Documentation
?randomForest
# With the default values 500 trees were built, and the model randomly sampled 3 predictors at each split.
# It also shows a matrix containing prediction vs actual, as well as classification error for each class.
# Testing the model on the test data set
pred <- predict(model, newdata = test)
pred_table <- table(pred, test$taste)
# Accuracy is calculated as follows:
(pred_table[1,1] + pred_table[2,2] + pred_table[3,3]) / nrow(test)
model <- randomForest(taste ~ .   pH + sulphates + chlorides, data = train)
model <- randomForest(taste ~ .  + pH + sulphates + chlorides, data = train)
# Show the model configuration
model
model <- randomForest(taste ~ . - quality, data = train, ntree=2000)
# Show the model configuration
model
pred <- predict(model, newdata = test)
pred_table <- table(pred, test$taste)
# Accuracy is calculated as follows:
(pred_table[1,1] + pred_table[2,2] + pred_table[3,3]) / nrow(test)
varImpPlot(model)
model <- randomForest(taste ~ . - quality + alcohol, data = train)
model
varImpPlot(model)
# We can use ntree and mtry to specify the total number of trees to build (default = 500),
# and the number of predictors to randomly sample at each split respectively. See the R Documentation
?randomForest
# With the default values 500 trees were built, and the model randomly sampled 3 predictors at each split.
# It also shows a matrix containing prediction vs actual, as well as classification error for each class.
# Testing the model on the test data set
pred <- predict(model, newdata = test)
pred_table <- table(pred, test$taste)
# Accuracy is calculated as follows:
(pred_table[1,1] + pred_table[2,2] + pred_table[3,3]) / nrow(test)
varImpPlot(model)
model <- randomForest(as.factor(taste) ~ . - quality + alcohol, data = train, importance=TRUE)
# Show the model configuration
model
# what variables were important
varImpPlot(model)
model <- randomForest(as.factor(taste) ~ . - quality + sulphates, data = train, importance=TRUE)
# Show the model configuration
model
model <- randomForest(as.factor(taste) ~ . - quality + sulphates, data = train, importance=TRUE, mtry=5)
# Show the model configuration
model
model <- randomForest(as.factor(taste) ~ . - quality + sulphates, data = train, importance=TRUE, mtry=7)
# Show the model configuration
model
model <- randomForest(as.factor(taste) ~ . - quality + sulphates, data = train, importance=TRUE, mtry=1)
# Show the model configuration
model
model <- randomForest(as.factor(taste) ~ . - quality + sulphates, data = train, importance=TRUE)
# Show the model configuration
model
head(iris)
names(iris)
x = iris[,-5]
y = iris$Species
# Build  the k means model. Note that the cluster number k is predefined
kc <- kmeans(x,3)
# Show the model summary
kc
# Result in table format
table(y,kc$cluster)
# Plot the results
plot(x[c("Sepal.Length", "Sepal.Width")], col=kc$cluster)
points(kc$centers[,c("Sepal.Length", "Sepal.Width")], col=1:3, pch=23, cex=3)
kc <- kmeans(x,2)
# Show the model summary
kc
# Result in table format
table(y,kc$cluster)
# Plot the results
plot(x[c("Sepal.Length", "Sepal.Width")], col=kc$cluster)
points(kc$centers[,c("Sepal.Length", "Sepal.Width")], col=1:3, pch=23, cex=3)
kc <- kmeans(x,4)
# Show the model summary
kc
# Result in table format
table(y,kc$cluster)
# Plot the results
plot(x[c("Sepal.Length", "Sepal.Width")], col=kc$cluster)
points(kc$centers[,c("Sepal.Length", "Sepal.Width")], col=1:3, pch=23, cex=3)
library(ggplot2)
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
?kmeans
head(iris)
names(iris)
library(ggplot2)
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
set.seed(20)
irisCluster <- kmeans(iris[, 3:4], centers=3, nstart = 20)
irisCluster
table(irisCluster$cluster, iris$Species)
irisCluster$cluster <- as.factor(irisCluster$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color = iris$cluster)) + geom_point()
head(iris)
library(ggplot2)
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
# Show the docu for kmeans
?kmeans
set.seed(418)
# Build  the k means model. Note that the cluster number k is predefined
irisCluster <- kmeans(iris[, 3:4], centers=3, nstart = 20)
# Show the model summary
irisCluster
# Result in table format
table(irisCluster$cluster, iris$Species)
irisCluster$cluster <- as.factor(irisCluster$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color = iris$cluster)) + geom_point()
iris
ggplot(iris, aes(subset(iris,Petal.Length), subset(iris,Petal.Width), color = iris$cluster)) + geom_point()
ggplot(iris, aes(x=Petal.Length, y=Petal.Width, color = iris$cluster)) + geom_point()
iris$cluster <- as.factor(irisCluster$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color = iris$cluster)) + geom_point()
irisCluster$centers
ggplot(iris, aes(Petal.Length, Petal.Width, color = iris$cluster)) +
geom_point() +
geom_point(data = irisCluster$centers[,c("Sepal.Length", "Sepal.Width")], colour = "red")
geom_point(data = irisCluster$centers[,c("Petal.Length", "Petal.Width")], colour = "red")
ggplot(iris, aes(Petal.Length, Petal.Width, color = iris$cluster)) +
geom_point() +
geom_point(data = irisCluster$centers[,c("Petal.Length", "Petal.Width")], colour = "red")
irisCluster$centers[,1]
ggplot(iris, aes(Petal.Length, Petal.Width, color = iris$cluster)) + geom_point() +
geom_point(aes(x = irisCluster$centers[,1], y = irisCluster$centers[,2]))
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = iris$cluster)) + geom_point()
geom_point(aes(x = irisCluster$centers[,1], y = irisCluster$centers[,2]))
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = iris$cluster)) + geom_point() +
geom_point(aes(x = irisCluster$centers[,1], y = irisCluster$centers[,2]))
head(iris)
library(ggplot2)
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
# Show the docu for kmeans
?kmeans
set.seed(418)
# Build  the k means model. Note that the cluster number k is predefined
irisCluster <- kmeans(iris[, 3:4], centers=3, nstart = 20)
# Show the model summary
irisCluster
# Results in table format
table(irisCluster$cluster, iris$Species)
irisCluster$centers
iris$cluster <- as.factor(irisCluster$cluster)
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = iris$cluster)) + geom_point()
data(iris)
head(iris, 3)
# first log transform the data
log.ir <- log(iris[, 1:4])
ir.species <- iris[, 5]
# apply PCA
# scale. = TRUE is highly advisable, but default is FALSE.
ir.pca <- prcomp(log.ir,
center = TRUE,
scale. = TRUE)
# print method
print(ir.pca)
# plot method
plot(ir.pca, type = "l")
# summary method
summary(ir.pca)
# Predict PCs, assuming the last two raws of the data are new
predict(ir.pca, newdata=tail(log.ir, 2))
# Plot the PC1 and PC2 values
biplot(ir.pca)
# There is a nicer version of this plot on github
library(devtools)
library(ggbiplot)
g <- ggbiplot(ir.pca, obs.scale = 1, var.scale = 1,
groups = ir.species, ellipse = TRUE,
circle = TRUE)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal',
legend.position = 'top')
print(g)
biplot(ir.pca)
iris
summary(iris)
str(iris)
table(iris$Species)
round(prop.table(table(iris$Species)) * 100,digits = 1)
ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.7, 0.3))
trainData <- iris[ind==1,]
testData <- iris[ind==2,]
trainData1 <- trainData[-5]
testData1 <- testData[-5]
dim(trainData)
dim(trainData1)
dim(testData)
dim(testData1)
iris_train_labels <- trainData$Species
dim(iris_train_labels)
class(iris_train_labels)
iris_test_labels <- testData$Species
dim(iris_test_labels)
class(iris_test_labels)
library(class)
iris_test_pred1 <- knn(train = trainData1, test = testData1, cl= iris_train_labels,k = 3,prob=TRUE)
knn
n <- 150 # number of data points
p <- 2   # dimension
sigma <- 1  # variance of the distribution
meanpos <- 0 # centre of the distribution of positive examples
meanneg <- 3 # centre of the distribution of negative examples
npos <- round(n/2) # number of positive examples
nneg <- n-npos # number of negative examples
xpos <- matrix(rnorm(npos*p,mean=meanpos,sd=sigma),npos,p)
xneg <- matrix(rnorm(nneg*p,mean=meanneg,sd=sigma),npos,p)
x <- rbind(xpos,xneg)
y <- matrix(c(rep(1,npos),rep(-1,nneg)))
plot(x,col=ifelse(y>0,1,2))
legend("topleft",c('Positive','Negative'),col=seq(2),pch=1,text.col=seq(2))
ntrain <- round(n*0.8) # number of training examples
tindex <- sample(n,ntrain) # indices of training samples
xtrain <- x[tindex,]
xtest <- x[-tindex,]
ytrain <- y[tindex]
ytest <- y[-tindex]
istrain=rep(0,n)
istrain[tindex]=1
plot(x,col=ifelse(y>0,1,2),pch=ifelse(istrain==1,1,2))
legend("topleft",c('Positive Train','Positive Test','Negative Train','Negative Test'),
col=c(1,1,2,2),pch=c(1,2,1,2),text.col=c(1,1,2,2))
library(kernlab)
svp <- ksvm(xtrain,ytrain,type="C-svc",kernel='vanilladot',C=100,scaled=c())
svp
svp
attributes(svp)
alpha(svp)
alphaindex(svp)
b(svp)
plot(svp,data=xtrain)
ypred = predict(svp,xtest)
table(ytest,ypred)
sum(ypred==ytest)/length(ytest)
ypredscore = predict(svp,xtest,type="decision")
table(ypredscore > 0,ypred)
library(ROCR)
pred <- prediction(ypredscore,ytest)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf)
perf <- performance(pred, measure = "prec", x.measure = "rec")
plot(perf)
perf <- performance(pred, measure = "acc")
plot(perf)
wine <- read.csv("data/winequality-white.csv", sep=";")
head(wine)
wine
barplot(table(wine$quality))
wine$taste <- ifelse(wine$quality < 6, 'bad', 'good')
wine$taste[wine$quality == 6] <- 'normal'
wine$taste <- as.factor(wine$taste)
table(wine$taste)
set.seed(123)
samp <- sample(nrow(wine), 0.66 * nrow(wine))
train <- wine[samp, ]
test <- wine[-samp, ]
library(randomForest)
model <- randomForest(as.factor(taste) ~ . - quality + sulphates, data = train, importance=TRUE)
model
varImpPlot(model)
?randomForest
pred <- predict(model, newdata = test)
pred_table <- table(pred, test$taste)
(pred_table[1,1] + pred_table[2,2] + pred_table[3,3]) / nrow(test)
head(iris)
library(ggplot2)
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
?kmeans
set.seed(418)
irisCluster <- kmeans(iris[, 3:4], centers=3, nstart = 20)
irisCluster
table(irisCluster$cluster, iris$Species)
irisCluster$centers
iris$cluster <- as.factor(irisCluster$cluster)
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = iris$cluster)) + geom_point()
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
irisCluster <- kmeans(iris[, 3:4], centers=4, nstart = 20)
irisCluster
table(irisCluster$cluster, iris$Species)
irisCluster$centers
iris$cluster <- as.factor(irisCluster$cluster)
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = iris$cluster)) + geom_point()
irisCluster <- kmeans(iris[, 3:4], centers=3, nstart = 200)
irisCluster
table(irisCluster$cluster, iris$Species)
irisCluster$centers
data(iris)
head(iris, 3)
log.ir <- log(iris[, 1:4])
ir.species <- iris[, 5]
ir.pca <- prcomp(log.ir,
center = TRUE,
scale. = TRUE)
print(ir.pca)
plot(ir.pca, type = "l")
plot(ir.pca, type = "l")
summary(ir.pca)
predict(ir.pca, newdata=tail(log.ir, 2))
biplot(ir.pca)
library(ggbiplot)
g <- ggbiplot(ir.pca, obs.scale = 1, var.scale = 1,
groups = ir.species, ellipse = TRUE,
circle = TRUE)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal',
legend.position = 'top')
print(g)
